{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:29:01.889174Z",
     "start_time": "2021-06-10T21:29:00.656080Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import geometry\n",
    "from pyproj import CRS\n",
    "import geog\n",
    "import shapely\n",
    "from shapely.geometry import Point\n",
    "import shapely.wkt\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the study, we merge all the data into complete datasets. What is worth mentioning is that we collect data for 2 cities: Warszawa and Kraków, both for 1km2 grids and 500m buffers around Inpost points (only for GWR). In the case of merging for grids we took an assumption about intersection of grid with poviat (it was enough for grids to intersect with poviat boundaries). Additionally, we count other data for each grid. In turn, in the case of merging for buffers, we also count data for each buffer and also we use data from Inspire (if more than one grid intersects with buffer we take avarage value from those grids)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parcel data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we merge the files for parcel points. Finally, we get a file that contains geolocation data for all available pickup points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:29:02.128003Z",
     "start_time": "2021-06-10T21:29:01.891172Z"
    }
   },
   "outputs": [],
   "source": [
    "bliska_paczka = pd.read_csv(\"../datasets/raw_data/bliska_paczka.csv\", index_col=0)\n",
    "bliska_paczka = bliska_paczka.loc[bliska_paczka.available == True, (\"brand\", \"operator\", \"city\", \"street\", \"longitude\", \"latitude\")]\n",
    "bliska_paczka.brand = bliska_paczka.brand.str.lower()\n",
    "bliska_paczka.operator = bliska_paczka.operator.str.lower()\n",
    "bliska_paczka.city = bliska_paczka.city.str.lower()\n",
    "bliska_paczka.street = bliska_paczka.street.str.lower()\n",
    "bliska_paczka = bliska_paczka.loc[:, (\"brand\", \"operator\", \"longitude\", \"latitude\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:29:02.165520Z",
     "start_time": "2021-06-10T21:29:02.131795Z"
    }
   },
   "outputs": [],
   "source": [
    "dhl = pd.read_csv(\"../datasets/raw_data/dhl.csv\", index_col=0)\n",
    "dhl.P_TYPE = dhl.P_TYPE.str.lower() \n",
    "dhl = dhl.drop(columns=[\"ID\"])\n",
    "dhl.columns = [\"brand\", \"latitude\", \"longitude\"]\n",
    "dhl[\"operator\"] = \"dhl\"\n",
    "dhl = dhl[[\"brand\", \"operator\", \"longitude\", \"latitude\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:29:02.176493Z",
     "start_time": "2021-06-10T21:29:02.167518Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([bliska_paczka, dhl], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:29:02.938511Z",
     "start_time": "2021-06-10T21:29:02.276235Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"brand\"])\n",
    "df = gpd.GeoDataFrame(df, geometry=[geometry.Point(xy) for xy in zip(df.longitude, df.latitude)], crs=CRS(\"epsg:4258\"))\n",
    "df = df.drop(columns=[\"latitude\", \"longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:29:10.327311Z",
     "start_time": "2021-06-10T21:29:08.293064Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('../datasets/preprocessed_data/pickup_points_by_operator.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GUGIK and Inspire data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we load data for GUGIK and Inspire. GUGIK data is necessary to obtain polygons for 2 poviats (Warszawa and Kraków). In turn, Inspire data provides us with demographic variables that define the population, both total and distinguished by gender and age for 1 km2 grids. We merge both of the above data sets by location, i.e. grids within and intersecting with poviats remain in our data set. Similarly, we add data for parcels of points according to their presence within poviats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:30:39.114758Z",
     "start_time": "2021-06-10T21:29:10.330304Z"
    }
   },
   "outputs": [],
   "source": [
    "pov = gpd.read_file(\"../datasets/raw_data/guigk_pov/Powiaty.shx\", encoding='utf-8')\n",
    "pov = pov.loc[pov.JPT_NAZWA_.isin([\"powiat Warszawa\", \"powiat Kraków\"])==True, (\"JPT_NAZWA_\", \"geometry\")]\n",
    "pov = pov.to_crs(\"epsg:4258\")\n",
    "\n",
    "grids = gpd.read_file(\"../datasets/raw_data/inspire/PD_STAT_GRID_CELL_2011.shp\", encoding='utf-8')\n",
    "grids = grids[['TOT', 'TOT_0_14', 'TOT_15_64', 'TOT_65__', 'TOT_MALE', 'TOT_FEM',\n",
    "       'MALE_0_14', 'MALE_15_64', 'MALE_65__', 'FEM_0_14', 'FEM_15_64',\n",
    "       'FEM_65__', 'FEM_RATIO', 'geometry']]\n",
    "grids = grids.to_crs(\"epsg:4258\")\n",
    "\n",
    "pov_grids = gpd.sjoin(grids, pov, how=\"inner\", op=\"intersects\")\n",
    "pov_grids = pov_grids.drop(columns=[\"index_right\"])\n",
    "pov_grids = pov_grids.reset_index()\n",
    "pov_grids = pov_grids.rename(columns={\"index\":\"grid_index\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:30:42.134653Z",
     "start_time": "2021-06-10T21:30:39.653924Z"
    }
   },
   "outputs": [],
   "source": [
    "pov_grids_sliced = pov_grids[[\"grid_index\", \"geometry\"]].copy() \n",
    "df_pov_grids_sliced = gpd.sjoin(df, pov_grids_sliced, how=\"right\", op=\"within\")\n",
    "df_pov_grids_sliced = df_pov_grids_sliced.drop(columns=[\"index_left\"])\n",
    "df_pov_grids_sliced = df_pov_grids_sliced.groupby([\"operator\", \"grid_index\"], as_index=False).count()\n",
    "df_pov_grids_sliced = df_pov_grids_sliced.pivot(index='grid_index', columns='operator').fillna(0).reset_index()\n",
    "df_pov_grids_sliced.columns = df_pov_grids_sliced.columns.droplevel()\n",
    "df_pov_grids_sliced.columns = ['grid_index', 'dhl', 'dpd', 'fedex', 'inpost', 'poczta', 'ruch', 'ups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:30:42.195253Z",
     "start_time": "2021-06-10T21:30:42.136405Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pov_grids = pd.merge(pov_grids, df_pov_grids_sliced, how=\"left\", on=\"grid_index\")\n",
    "df_pov_grids.columns = df_pov_grids.columns.str.lower()\n",
    "df_pov_grids = df_pov_grids.fillna(0)\n",
    "df_pov_grids = df_pov_grids[['grid_index','geometry', 'jpt_nazwa_', 'dhl', 'dpd', 'fedex', 'inpost', 'poczta', 'ruch',\n",
    "                             'ups', 'tot', 'tot_0_14', 'tot_15_64', 'tot_65__', 'tot_male',\n",
    "                            'tot_fem', 'male_0_14', 'male_15_64', 'male_65__', 'fem_0_14',\n",
    "                            'fem_15_64', 'fem_65__', 'fem_ratio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:30:42.909918Z",
     "start_time": "2021-06-10T21:30:42.835537Z"
    }
   },
   "outputs": [],
   "source": [
    "del bliska_paczka, dhl, pov_grids, pov_grids_sliced, df_pov_grids_sliced; gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSM data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in order to merge OSM (points of interest) variables to the final dataset, we created a function that loads OSM data for voivodeships. Then we count the points of interest for each grid. Thus, we obtain the final dataset for grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_vars(data, poviat, path):\n",
    "    \n",
    "    # df for concrete poviat\n",
    "    df_output = data[data.jpt_nazwa_ == poviat]\n",
    "    \n",
    "    # loading points of interest\n",
    "    buildings = gpd.read_file(path + 'gis_osm_buildings_a_free_1.shp')\n",
    "    landuse = gpd.read_file(path + 'gis_osm_landuse_a_free_1.shp')\n",
    "    pois_a = gpd.read_file(path + 'gis_osm_pois_a_free_1.shp')\n",
    "    railways = gpd.read_file(path + 'gis_osm_railways_free_1.shp')\n",
    "    roads = gpd.read_file(path + 'gis_osm_roads_free_1.shp')\n",
    "    traffic_a = gpd.read_file(path + 'gis_osm_traffic_a_free_1.shp')\n",
    "    traffic = gpd.read_file(path + 'gis_osm_traffic_free_1.shp')\n",
    "    transport_a = gpd.read_file(path + 'gis_osm_transport_a_free_1.shp')\n",
    "    \n",
    "    # concrete points from above dfs\n",
    "    buildings_points = buildings[buildings['type'].isin(['house', 'residential', 'bungalow', 'apartment'])][['osm_id', 'geometry']]\n",
    "    shop_points = buildings[buildings['type'].isin(['supermarket', 'bakery', 'kiosk', 'mall', 'department_store', 'convenience', 'clothes', 'florist', 'chemist'])][['osm_id', 'geometry']]\n",
    "    parks_points = landuse[landuse['fclass'] == 'park'][['osm_id', 'geometry']]\n",
    "    forest_points = landuse[landuse['fclass'] == 'forest'][['osm_id', 'geometry']]\n",
    "    schools_points = pois_a[pois_a['fclass'].isin(['school', 'playground'])][['osm_id', 'geometry']]\n",
    "    railways_points = railways[['osm_id', 'geometry']]\n",
    "    cycleways_points = roads[roads['fclass'] == 'cycleway'][['osm_id', 'geometry']]\n",
    "    parking_points = traffic_a[traffic_a['fclass'] == 'parking'][['osm_id', 'geometry']]\n",
    "    crossing_points = traffic[traffic['fclass'] == 'crossing'][['osm_id', 'geometry']]\n",
    "    bus_stop_points = transport_a[transport_a['fclass'] == 'bus_stop'][['osm_id', 'geometry']]\n",
    "    \n",
    "    # unnecessery dfs\n",
    "    del buildings, landuse, pois_a, railways, roads, traffic_a, traffic, transport_a; gc.collect()\n",
    "    \n",
    "    # changing crs\n",
    "    buildings_points = buildings_points.to_crs(\"epsg:4258\")\n",
    "    shop_points = shop_points.to_crs(\"epsg:4258\")\n",
    "    parks_points = parks_points.to_crs(\"epsg:4258\")\n",
    "    forest_points = forest_points.to_crs(\"epsg:4258\")\n",
    "    schools_points = schools_points.to_crs(\"epsg:4258\")\n",
    "    railways_points = railways_points.to_crs(\"epsg:4258\")\n",
    "    cycleways_points = cycleways_points.to_crs(\"epsg:4258\")\n",
    "    parking_points = parking_points.to_crs(\"epsg:4258\")\n",
    "    crossing_points = crossing_points.to_crs(\"epsg:4258\")\n",
    "    bus_stop_points = bus_stop_points.to_crs(\"epsg:4258\")\n",
    "    \n",
    "    # list of dataframes\n",
    "    list_of_dfs = [buildings_points, shop_points, parks_points, forest_points, schools_points, railways_points,\n",
    "                  cycleways_points, parking_points, crossing_points, bus_stop_points]\n",
    "    \n",
    "    # names of new columns\n",
    "    names = ['buildings', 'shops', 'parks', 'forests', 'schools', 'railways',\n",
    "                  'cycleways', 'parkings', 'crossings', 'bus_stops']\n",
    "    \n",
    "    # groupby points in a loop\n",
    "    for i in range(len(list_of_dfs)):\n",
    "        actual_point = gpd.sjoin(list_of_dfs[i], df_output, how=\"inner\", op=\"intersects\")\n",
    "        x = actual_point[['osm_id', 'grid_index']].groupby(['grid_index']).count()\n",
    "        x.rename(columns={\"osm_id\": names[i]}, inplace=True)\n",
    "        x.reset_index(inplace = True)\n",
    "        df_output = df_output.merge(x, on = 'grid_index', how='outer')\n",
    "        \n",
    "    df_output.fillna(0, inplace=True)\n",
    "    df_output.drop(columns = {'jpt_nazwa_'}, inplace = True)\n",
    "\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving final dataset for grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_krakow = merging_vars(df_pov_grids, 'powiat Kraków', '../datasets/raw_data/osm_malopolskie/')\n",
    "df_krakow[['tot', 'tot_0_14', 'tot_15_64', 'tot_65__', 'tot_male', 'tot_fem',\n",
    "       'male_0_14', 'male_15_64', 'male_65__', 'fem_0_14', 'fem_15_64',\n",
    "       'fem_65__']] = df_krakow[['tot', 'tot_0_14', 'tot_15_64', 'tot_65__', 'tot_male', 'tot_fem',\n",
    "       'male_0_14', 'male_15_64', 'male_65__', 'fem_0_14', 'fem_15_64',\n",
    "       'fem_65__']].astype(float)\n",
    "\n",
    "df_krakow.to_csv('../datasets/preprocessed_data/df_krakow.csv', index = False)\n",
    "df_krakow.to_file('../datasets/preprocessed_data/df_krakow.shp')\n",
    "del df_krakow; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_warszawa = merging_vars(df_pov_grids, 'powiat Warszawa', '../datasets/raw_data/osm_mazowieckie/')\n",
    "df_warszawa[['tot', 'tot_0_14', 'tot_15_64', 'tot_65__', 'tot_male', 'tot_fem',\n",
    "       'male_0_14', 'male_15_64', 'male_65__', 'fem_0_14', 'fem_15_64',\n",
    "       'fem_65__']] = df_warszawa[['tot', 'tot_0_14', 'tot_15_64', 'tot_65__', 'tot_male', 'tot_fem',\n",
    "       'male_0_14', 'male_15_64', 'male_65__', 'fem_0_14', 'fem_15_64',\n",
    "       'fem_65__']].astype(float)\n",
    "\n",
    "df_warszawa.to_csv('../datasets/preprocessed_data/df_warszawa.csv', index = False)\n",
    "df_warszawa.to_file('../datasets/preprocessed_data/df_warszawa.shp')\n",
    "del df_warszawa; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for GWR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When constructing the dataset for GWR, we followed the same steps as for the above dataset. The only difference is that we collected data for 500m buffers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:39:06.163986Z",
     "start_time": "2021-06-10T21:39:06.145034Z"
    }
   },
   "outputs": [],
   "source": [
    "df_geo = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:39:06.620715Z",
     "start_time": "2021-06-10T21:39:06.593270Z"
    }
   },
   "outputs": [],
   "source": [
    "df_geo['point_id'] = list(range(0, df_geo.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:39:10.971894Z",
     "start_time": "2021-06-10T21:39:08.672124Z"
    }
   },
   "outputs": [],
   "source": [
    "pov_df_geo = gpd.sjoin(pov, df_geo, how=\"inner\", op=\"intersects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:39:22.259636Z",
     "start_time": "2021-06-10T21:39:22.191711Z"
    }
   },
   "outputs": [],
   "source": [
    "pov_df_geo = pov_df_geo.merge(df_geo, on = 'point_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:39:36.297223Z",
     "start_time": "2021-06-10T21:39:36.288283Z"
    }
   },
   "outputs": [],
   "source": [
    "pov_df_geo = pov_df_geo.drop(columns = {'geometry_x', 'index_right', 'operator_y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:39:36.871569Z",
     "start_time": "2021-06-10T21:39:36.864581Z"
    }
   },
   "outputs": [],
   "source": [
    "pov_df_geo = pov_df_geo.rename(columns={\"geometry_y\": \"geometry\", \"JPT_NAZWA_\": \"jpt_nazwa_\", \"operator_x\": \"operator\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:39:44.794106Z",
     "start_time": "2021-06-10T21:39:44.785134Z"
    }
   },
   "outputs": [],
   "source": [
    "inpost_points = pov_df_geo[pov_df_geo['operator'] == 'inpost']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating 500m buffer around each Inpost point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:39:45.450496Z",
     "start_time": "2021-06-10T21:39:45.441518Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating buffer (500m radius)\n",
    "def buffer(point):\n",
    "\n",
    "    n_points = 50\n",
    "    angles = np.linspace(0, 360, n_points)\n",
    "    radius = 500\n",
    "    polygon = geog.propagate(point, angles, radius)\n",
    "\n",
    "    x = polygon.tolist()\n",
    "    lon = list(list(zip(*x))[0])\n",
    "    lat = list(list(zip(*x))[1])\n",
    "    pts = gpd.GeoSeries([Point(x, y) for x, y in zip(lon, lat)])\n",
    "    poly = geometry.Polygon([[p.x, p.y] for p in pts])\n",
    "    polyg = gpd.GeoSeries(poly)\n",
    "    \n",
    "\n",
    "    return polyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:39:51.540661Z",
     "start_time": "2021-06-10T21:39:46.464415Z"
    }
   },
   "outputs": [],
   "source": [
    "inpost_points['buffer'] = inpost_points.apply(lambda x: buffer(x['geometry']), axis=1)\n",
    "inpost_points = inpost_points.rename(columns={\"geometry\": \"center\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:39:55.271332Z",
     "start_time": "2021-06-10T21:39:55.247397Z"
    }
   },
   "outputs": [],
   "source": [
    "inpost_buffers = gpd.GeoDataFrame(inpost_points, geometry = 'buffer', crs = \"epsg:4258\")\n",
    "inpost_buffers = inpost_buffers.reset_index()\n",
    "inpost_buffers = inpost_buffers.rename(columns={\"index\":\"buffer_index\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parcel points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:30:53.661442Z",
     "start_time": "2021-06-10T21:30:53.652477Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_points(data_inpost, data_operator, operator):\n",
    "    new_df = data_operator[data_operator['operator'] == operator]\n",
    "    new_df = gpd.GeoDataFrame(new_df, geometry = 'geometry', crs = \"epsg:4258\")\n",
    "    \n",
    "    actual_point = gpd.sjoin(new_df, data_inpost, how=\"inner\", op=\"intersects\")\n",
    "    x = actual_point[['operator_left', 'buffer_index']].groupby(['buffer_index']).count()\n",
    "    x.rename(columns={\"operator_left\": operator+'_points'}, inplace=True)\n",
    "    x.reset_index(inplace = True)\n",
    "    df_output = data_inpost.merge(x, on = 'buffer_index', how='outer')\n",
    "    df_output.fillna(0, inplace=True)\n",
    "    \n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:30:57.253656Z",
     "start_time": "2021-06-10T21:30:53.666427Z"
    }
   },
   "outputs": [],
   "source": [
    "operators = ['inpost', 'poczta', 'dhl', 'ruch', 'dpd', 'ups', 'fedex']\n",
    "for operator in operators:\n",
    "    inpost_buffers = merge_points(inpost_buffers, pov_df_geo, operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:31:22.622181Z",
     "start_time": "2021-06-10T21:30:57.255654Z"
    }
   },
   "outputs": [],
   "source": [
    "buff = gpd.sjoin(inpost_buffers, grids, how=\"inner\", op=\"intersects\")\n",
    "\n",
    "x = buff[['buffer_index', 'TOT', 'TOT_0_14', 'TOT_15_64', 'TOT_65__', 'TOT_MALE', 'TOT_FEM',\n",
    "       'MALE_0_14', 'MALE_15_64', 'MALE_65__', 'FEM_0_14', 'FEM_15_64',\n",
    "       'FEM_65__', 'FEM_RATIO']].groupby(['buffer_index']).mean()\n",
    "\n",
    "inpost_buffers = inpost_buffers.merge(x, on='buffer_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:28:51.868227Z",
     "start_time": "2021-06-10T21:28:43.772Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_osm(data, poviat, path):\n",
    "    \n",
    "    # df for concrete poviat\n",
    "    df_output = data[data.jpt_nazwa_ == poviat]\n",
    "    \n",
    "    # loading points of interest\n",
    "    buildings = gpd.read_file(path + 'gis_osm_buildings_a_free_1.shp')\n",
    "    landuse = gpd.read_file(path + 'gis_osm_landuse_a_free_1.shp')\n",
    "    pois_a = gpd.read_file(path + 'gis_osm_pois_a_free_1.shp')\n",
    "    railways = gpd.read_file(path + 'gis_osm_railways_free_1.shp')\n",
    "    roads = gpd.read_file(path + 'gis_osm_roads_free_1.shp')\n",
    "    traffic_a = gpd.read_file(path + 'gis_osm_traffic_a_free_1.shp')\n",
    "    traffic = gpd.read_file(path + 'gis_osm_traffic_free_1.shp')\n",
    "    transport_a = gpd.read_file(path + 'gis_osm_transport_a_free_1.shp')\n",
    "    \n",
    "    # concrete points from above dfs\n",
    "    buildings_points = buildings[buildings['type'].isin(['house', 'residential', 'bungalow', 'apartment'])][['osm_id', 'geometry']]\n",
    "    shop_points = buildings[buildings['type'].isin(['supermarket', 'bakery', 'kiosk', 'mall', 'department_store', 'convenience', 'clothes', 'florist', 'chemist'])][['osm_id', 'geometry']]\n",
    "    parks_points = landuse[landuse['fclass'] == 'park'][['osm_id', 'geometry']]\n",
    "    forest_points = landuse[landuse['fclass'] == 'forest'][['osm_id', 'geometry']]\n",
    "    schools_points = pois_a[pois_a['fclass'].isin(['school', 'playground'])][['osm_id', 'geometry']]\n",
    "    railways_points = railways[['osm_id', 'geometry']]\n",
    "    cycleways_points = roads[roads['fclass'] == 'cycleway'][['osm_id', 'geometry']]\n",
    "    parking_points = traffic_a[traffic_a['fclass'] == 'parking'][['osm_id', 'geometry']]\n",
    "    crossing_points = traffic[traffic['fclass'] == 'crossing'][['osm_id', 'geometry']]\n",
    "    bus_stop_points = transport_a[transport_a['fclass'] == 'bus_stop'][['osm_id', 'geometry']]\n",
    "    \n",
    "    # unnecessery dfs\n",
    "    del buildings, landuse, pois_a, railways, roads, traffic_a, traffic, transport_a; gc.collect()\n",
    "    \n",
    "    # changing crs\n",
    "    buildings_points = buildings_points.to_crs(\"epsg:4258\")\n",
    "    shop_points = shop_points.to_crs(\"epsg:4258\")\n",
    "    parks_points = parks_points.to_crs(\"epsg:4258\")\n",
    "    forest_points = forest_points.to_crs(\"epsg:4258\")\n",
    "    schools_points = schools_points.to_crs(\"epsg:4258\")\n",
    "    railways_points = railways_points.to_crs(\"epsg:4258\")\n",
    "    cycleways_points = cycleways_points.to_crs(\"epsg:4258\")\n",
    "    parking_points = parking_points.to_crs(\"epsg:4258\")\n",
    "    crossing_points = crossing_points.to_crs(\"epsg:4258\")\n",
    "    bus_stop_points = bus_stop_points.to_crs(\"epsg:4258\")\n",
    "    \n",
    "    # list of dataframes\n",
    "    list_of_dfs = [buildings_points, shop_points, parks_points, forest_points, schools_points, railways_points,\n",
    "                  cycleways_points, parking_points, crossing_points, bus_stop_points]\n",
    "    \n",
    "    # names of new columns\n",
    "    names = ['buildings', 'shops', 'parks', 'forests', 'schools', 'railways',\n",
    "                  'cycleways', 'parkings', 'crossings', 'bus_stops']\n",
    "    \n",
    "    # groupby points in a loop\n",
    "    for i in range(len(list_of_dfs)):\n",
    "        actual_point = gpd.sjoin(list_of_dfs[i], df_output, how=\"inner\", op=\"intersects\")\n",
    "        x = actual_point[['osm_id', 'buffer_index']].groupby(['buffer_index']).count()\n",
    "        x.rename(columns={\"osm_id\": names[i]}, inplace=True)\n",
    "        x.reset_index(inplace = True)\n",
    "        df_output = df_output.merge(x, on = 'buffer_index', how='outer')\n",
    "        \n",
    "    df_output.fillna(0, inplace=True)\n",
    "    df_output.drop(columns = {'jpt_nazwa_'}, inplace = True)\n",
    "\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving final dataset for buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_krakow_inpost = merge_osm(inpost_buffers, 'powiat Kraków', '../datasets/raw_data/osm_malopolskie/')\n",
    "df_krakow_inpost.to_csv('../datasets/preprocessed_data/df_krakow_gwr.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_krakow_inpost; gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_warszawa_inpost = merge_osm(inpost_buffers, 'powiat Warszawa', '../datasets/raw_data/osm_mazowieckie/')\n",
    "df_warszawa_inpost.to_csv('../datasets/preprocessed_data/df_warszawa_gwr.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_warszawa_inpost; gc.collect();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
